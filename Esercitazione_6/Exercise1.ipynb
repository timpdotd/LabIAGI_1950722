{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/Sapienza-AI-Lab/esercitazione6-22-23/blob/main/Exercise1.ipynb\"><img align=\"left\" src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\" title=\"Open and Execute in Google Colaboratory\"></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Implementation\n",
    "In questo esercizio implementeremo la regressione logistica da zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.optimize as opt\n",
    "from sklearn import linear_model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Esercizio 1 - Sigmoid Function\n",
    "Iniziate implementando la funzione sigmoide, che è definita come:\n",
    "\n",
    "$$g(z) = \\frac{1}{1+e^{-z}}$$\n",
    "\n",
    "Nel nostro caso $z = \\theta^Tx$.\n",
    "\n",
    "Non usate cicli for, ma usate le funzioni di numpy per sfruttare il calcolo vettoriale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid Function\n",
    "def sigmoid(z):\n",
    "    raise NotImplementedError(\"You need to implement this function\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Esercizio 2 - Logistic Regression Cost Function\n",
    "Implementate la funzione di costo per la regressione logistica, che è definita come:\n",
    "\n",
    "$$J(\\theta) = -\\frac{1}{m}\\sum_{i=1}^{m}y^{(i)}\\log(h_\\theta(x^{(i)})) + (1-y^{(i)})\\log(1-h_\\theta(x^{(i)}))$$\n",
    "\n",
    "Usate la funzione sigmoide che avete implementato precedentemente e continuate a sfruttare la vettorizzazione."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression Cost Function\n",
    "def logistic_cost(W, X, Y):\n",
    "    raise NotImplementedError(\"You need to implement this function\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Esercizio 3 - Gradient Function (single step)\n",
    "\n",
    "Ora implementate la funzione che calcola il gradiente della funzione di costo. Il gradiente è definito come:\n",
    "\n",
    "$$\\frac{\\partial J(\\theta)}{\\partial \\theta_j} = \\frac{1}{m}\\sum_{i=1}^{m}(h_\\theta(x^{(i)}) - y^{(i)})x_j^{(i)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression Cost Gradient\n",
    "def cost_gradient(W, X, Y):\n",
    "    raise NotImplementedError(\"You need to implement this function\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Esercizio 4 - Prediction Function\n",
    "\n",
    "Implementate la funzione che calcola la predizione. La predizione è definita come:\n",
    "\n",
    "$$h_\\theta(x) = \\begin{cases} 1 & \\text{se } g(W^Tx) \\geq 0.5 \\\\ 0 & \\text{se } g(W^Tx) < 0.5 \\end{cases}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict Function\n",
    "def predict(W, X):\n",
    "    raise NotImplementedError(\"You need to implement this function\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Admission Dataset\n",
    "Usiamo l'Admission Dataset per testare le funzioni che abbiamo implementato. Il dataset contiene i risultati di due esami e la decisione di ammissione dei candidati in un'università."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "path = 'data/exercise1_data.txt'\n",
    "data = pd.read_csv(path, header=None, names=['Exam 1', 'Exam 2', 'Admitted'])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize data\n",
    "positive = data[data['Admitted'].isin([1])]\n",
    "negative = data[data['Admitted'].isin([0])]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "ax.scatter(positive['Exam 1'], positive['Exam 2'], s=50, c='b', marker='o', label='Admitted')\n",
    "ax.scatter(negative['Exam 1'], negative['Exam 2'], s=50, c='r', marker='x', label='Not Admitted')\n",
    "ax.legend()\n",
    "ax.set_xlabel('Exam 1 Score')\n",
    "ax.set_ylabel('Exam 2 Score')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up input and output matrices\n",
    "X = data[['Exam 1', 'Exam 2']].values\n",
    "m, n = X.shape\n",
    "X = np.concatenate((np.ones((m, 1)), X), axis=1)\n",
    "n += 1\n",
    "Y = np.array(data[['Admitted']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test sigmoid function\n",
    "z = np.linspace(-10, 10, 100)\n",
    "out = sigmoid(z)\n",
    "plt.figure()\n",
    "plt.plot(z, out)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test logistic cost function\n",
    "W = np.matrix(np.ones((3, 1))*0.1)\n",
    "print('Test cost function: ', logistic_cost(W, X, Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test logistic regression cost gradient\n",
    "result = opt.fmin_tnc(func=logistic_cost, x0=W, fprime=cost_gradient, args=(X, Y))\n",
    "print('Logistic cost after optimization: ', logistic_cost(result[0], X, Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict with computed weights\n",
    "Y_hat = predict(result[0], X)\n",
    "accuracy1 = 1.0/m * np.sum(Y_hat == Y.reshape((m,)))\n",
    "\n",
    "print(\"Accuracy with our implementation: \", accuracy1)\n",
    "\n",
    "# Test with sklearn\n",
    "logreg = linear_model.LogisticRegression(penalty=None)\n",
    "logreg.fit(X, Y.reshape((m,)).T)\n",
    "result2 = logreg.predict(X)\n",
    "accuracy2 = np.sum(result2 == Y.reshape((m,))) / m\n",
    "print(\"Accuracy with sklearn: \", accuracy2)\n",
    "\n",
    "# The two accuracies should be the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize decision boundary\n",
    "x1_min, x1_max = X[:, 1].min(), X[:, 1].max(),\n",
    "x2_min, x2_max = X[:, 2].min(), X[:, 2].max(),\n",
    "xx1, xx2 = np.meshgrid(np.linspace(x1_min, x1_max), np.linspace(x2_min, x2_max))\n",
    "h = sigmoid(np.c_[np.ones((xx1.ravel().shape[0], 1)), xx1.ravel(), xx2.ravel()].dot(result[0]))\n",
    "h = h.reshape(xx1.shape)\n",
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "ax.contour(xx1, xx2, h, [0.5], linewidths=1, colors='g')\n",
    "ax.scatter(positive['Exam 1'], positive['Exam 2'], s=50, c='b', marker='o', label='Admitted')\n",
    "ax.scatter(negative['Exam 1'], negative['Exam 2'], s=50, c='r', marker='x', label='Not Admitted')\n",
    "ax.legend()\n",
    "ax.set_xlabel('Exam 1 Score')\n",
    "ax.set_ylabel('Exam 2 Score')\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
